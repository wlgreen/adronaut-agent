# Campaign Setup Agent

An intelligent autonomous agent that transforms campaign data into optimized advertising strategies with continuous learning and experimentation.

## Features

- **Intelligent Routing**: LLM-based decision making determines whether to initialize, reflect, or enrich based on uploaded data
- **Auto-Discovery**: Minimal input required - agent infers product context, competitors, and benchmarks
- **Sequential Experiments**: Designs and documents 3-week experiment plans (platform, audience, creative tests)
- **Multi-Session Support**: Full context persistence across sessions for long-running optimization cycles
- **Complete Configs**: Generates ready-to-execute campaign configurations for TikTok and Meta platforms
- **Iterative Optimization**: Analyzes experiment results and automatically generates patch strategies

## Architecture

Built on:
- **LangGraph**: Orchestrates multi-step agent workflow with intelligent routing
- **Gemini 2.0 Flash**: Powers reasoning, strategy generation, and decision-making
- **Supabase**: PostgreSQL database for persistent state and session management
- **Tavily**: Web search for market benchmarks and competitive intelligence

## Setup

### 1. Prerequisites

- Python 3.10+
- Supabase account
- Gemini API key
- Tavily API key (optional, for web search)

### 2. Install Dependencies

```bash
# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install packages
pip install -r requirements.txt
```

### 3. Configure Environment

```bash
# Copy example env file
cp .env.example .env

# Edit .env with your credentials
nano .env
```

Required environment variables:
```
GEMINI_API_KEY=your_gemini_api_key
SUPABASE_URL=https://your-project.supabase.co
SUPABASE_KEY=your_supabase_anon_key
TAVILY_API_KEY=your_tavily_key  # Optional
```

### 4. Setup Database

1. Go to your Supabase project SQL Editor
2. Run the schema from `src/database/schema.sql`
3. Verify tables created: `projects`, `sessions`, `react_cycles`

## Usage

### Basic Workflow

```bash
# Session 1: Initial setup with historical data
python cli.py run --project-id eco-bottle-001
> Files: data/historical_campaigns.csv

# Session 2: Upload experiment results (after running campaigns)
python cli.py run --project-id eco-bottle-001
> Files: data/week1_results.csv

# Session 3: Continue optimization
python cli.py run --project-id eco-bottle-001
> Files: data/week2_results.csv
```

### File Formats

The agent automatically detects file types based on columns:

**Historical Campaign Data** (CSV/JSON):
```csv
campaign_name,spend,conversions,cpa,ctr,roas,impressions,clicks
Summer Campaign,1500,75,20.0,2.5,3.5,50000,1250
```

**Experiment Results** (CSV/JSON):
```csv
experiment_id,variant,date,spend,conversions,cpa,ctr,roas
exp_001,TikTok,2024-01-15,300,18,16.67,3.2,4.1
exp_001,Meta,2024-01-15,200,10,20.00,2.1,3.0
```

**Enrichment Data** (CSV/JSON):
```csv
competitor,market,benchmark_cpa,benchmark_ctr
Competitor A,Water Bottles,22.5,2.8
```

### How It Works

1. **Upload Files**: Agent analyzes file type (historical, experiment results, enrichment)
2. **Intelligent Routing**: LLM decides next action:
   - **Initialize**: New project â†’ data collection â†’ strategy â†’ campaign setup
   - **Reflect**: Experiment results â†’ performance analysis â†’ optimization patches
   - **Enrich**: Additional data â†’ strategy update â†’ config adjustment
3. **Execute Flow**: Runs appropriate node sequence
4. **Save State**: Persists everything to database
5. **Output**: Complete campaign configuration saved as JSON

### Example Output

```
============================================================
  Session Results
============================================================

Project ID: eco-bottle-001
Session: 2
Decision: reflect
Phase: optimizing
Iteration: 1

--- Messages ---
  â€¢ Loaded existing project: eco-bottle-001
  â€¢ Analyzed week1_results.csv: experiment_results, 10 rows
  â€¢ Router decision: reflect
  â€¢ Performance below threshold, optimizing...
  â€¢ Configuration updated to v1

--- Campaign Configuration ---
  Total Daily Budget: $500
  Experiment: Platform allocation optimization

  TikTok:
    Budget: $350
    Objective: CONVERSIONS

  Meta:
    Budget: $150
    Objective: CONVERSIONS

âœ“ Configuration saved to: campaign_eco-bottle-001_v1.json

--- Key Insights ---
  â€¢ TikTok outperformed Meta by 20% on CPA
  â€¢ Eco-conscious messaging resonates best
  â€¢ Target audience: 25-34 year olds

============================================================
```

## Project Structure

```
adronaut-agent/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ agent/          # LangGraph agent logic
â”‚   â”‚   â”œâ”€â”€ graph.py    # Workflow assembly
â”‚   â”‚   â”œâ”€â”€ nodes.py    # Core agent nodes
â”‚   â”‚   â”œâ”€â”€ router.py   # Intelligent routing
â”‚   â”‚   â””â”€â”€ state.py    # State definitions
â”‚   â”œâ”€â”€ modules/        # Business logic
â”‚   â”‚   â”œâ”€â”€ campaign.py         # Config generation
â”‚   â”‚   â”œâ”€â”€ data_loader.py      # File parsing
â”‚   â”‚   â”œâ”€â”€ insight.py          # Strategy builder
â”‚   â”‚   â”œâ”€â”€ reflection.py       # Performance analysis
â”‚   â”‚   â””â”€â”€ execution_planner.py # Timeline generation
â”‚   â”œâ”€â”€ database/       # Persistence layer
â”‚   â”‚   â”œâ”€â”€ client.py           # Supabase client
â”‚   â”‚   â”œâ”€â”€ persistence.py      # CRUD operations
â”‚   â”‚   â””â”€â”€ schema.sql          # Database schema
â”‚   â”œâ”€â”€ storage/        # File management
â”‚   â”‚   â””â”€â”€ file_manager.py     # Supabase storage I/O
â”‚   â”œâ”€â”€ llm/            # LLM integration
â”‚   â”‚   â””â”€â”€ gemini.py           # Gemini wrapper
â”‚   â””â”€â”€ utils/          # Utilities
â”‚       â””â”€â”€ progress.py         # Progress tracking
â”œâ”€â”€ examples/           # Sample data & outputs
â”‚   â”œâ”€â”€ sample_historical_data.csv
â”‚   â”œâ”€â”€ sample_experiment_results.csv
â”‚   â””â”€â”€ sample_outputs/
â”œâ”€â”€ cli.py              # CLI interface
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ ARCHITECTURE.md     # Technical documentation
â”œâ”€â”€ CONTRIBUTING.md     # Contribution guidelines
â””â”€â”€ .env
```

## Example Outputs

### Session Output

When you run the agent, you'll see detailed progress and results:

```
============================================================
  Session Results
============================================================

Project ID: eco-bottle-campaign-001
Session: 1
Decision: initialize
Phase: awaiting_results
Iteration: 0

--- Messages ---
  â€¢ New project: eco-bottle-campaign-001
  â€¢ Analyzed historical_q3.csv: historical, 45 rows
  â€¢ Router decision: initialize
  â€¢ Discovered 5 facts via LLM inference
  â€¢ Generated strategy with 8 insights
  â€¢ Created execution timeline: 14 days
  â€¢ Configuration generated successfully

--- Knowledge Graph ---
Total facts: 5

  product_description         Eco-friendly water bottles...  [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  ] 0.85 (llm_inference)
  target_budget               1000.0                         [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ] 0.95 (user_input)
  target_cpa                  25.0                           [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  ] 0.80 (llm_inference)
  target_roas                 3.0                            [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    ] 0.70 (llm_inference)
  market_segment              Health & Wellness              [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ ] 0.90 (llm_inference)

--- Campaign Configuration ---
  Total Daily Budget: $1000
  Experiment: Short-term Discovery (Days 1-5)

  TikTok:
    Budget: $600
    Objective: CONVERSIONS

  Meta:
    Budget: $400
    Objective: CONVERSIONS

âœ“ Configuration saved to: campaign_eco-bottle-campaign-001_v0.json
```

### Strategy Output

```json
{
  "insights": {
    "patterns": [
      "TikTok campaigns achieved 23% lower CPA ($18 vs $23) compared to Meta",
      "Female audience (25-34) showed 40% higher engagement rate",
      "UGC video content drove 3.2x ROAS vs static images (2.1x)"
    ],
    "strengths": [
      "Strong brand awareness in wellness community",
      "Proven conversion funnel with 8.5% checkout rate"
    ],
    "weaknesses": [
      "Meta platform underperforming on CPA targets",
      "Limited creative variety tested (only 3 formats)"
    ]
  },
  "target_audience": {
    "primary_segments": ["Health-conscious millennials", "Eco-warriors", "Fitness enthusiasts"],
    "demographics": {
      "age": "25-34",
      "gender": "Female-primary, All-secondary",
      "location": "United States (Urban areas)"
    },
    "interests": ["Sustainability", "Fitness", "Wellness", "Yoga", "Nutrition"]
  },
  "platform_strategy": {
    "priorities": ["TikTok", "Meta"],
    "budget_split": {"TikTok": 0.6, "Meta": 0.4},
    "rationale": "TikTok demonstrated 23% lower CPA and higher engagement with target demo"
  }
}
```

### Execution Timeline Output

```
============================================================
  EXECUTION TIMELINE (14 DAYS)
============================================================

ðŸ’¡ Timeline Design:
  14-day timeline chosen to allow 2 optimization cycles with
  sufficient statistical power. Budget supports 20+ conversions
  per combination for 90% confidence.

ðŸ“… TESTING PHASES (3 phases):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

[1] SHORT-TERM DISCOVERY
    Days 1-5 (5 days) | Budget: 35%
    Objectives:
      â€¢ Identify winning platform
      â€¢ Test 3 audience segments
      â€¢ Validate UGC vs static creative
    Test Combinations (3):
      [15%] TikTok + Interest + UGC Video
           â†’ Historical winner, testing scalability
      [12%] Meta + Lookalike + Static Image
           â†’ Hedge bet for audience discovery
      [8%] TikTok + Lookalike + UGC Video
           â†’ Testing interaction effect
    Success Criteria:
      âœ“ CPA < $30 in at least 1 combination
      âœ“ ROAS > 2.5x overall
    Decision Triggers:
      â†’ Proceed if: CPA < $30 in 2+ combos
      âš  Pause if: CPA > $50 across all

[2] MEDIUM-TERM VALIDATION
    Days 6-11 (6 days) | Budget: 40%
    ...

ðŸ“ CHECKPOINT SCHEDULE (3 checkpoints):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  ðŸŸ¡ Day 3: Early Signal Check
     Focus:
       â€¢ Check for obvious losers
       â€¢ Validate tracking setup
       â€¢ Confirm conversion volume

  ðŸ”´ Day 7: Phase 1 Decision Point
     Focus:
       â€¢ Identify winning combinations
       â€¢ Calculate statistical significance
       â€¢ Decide on phase 2 allocation

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  â±ï¸  Total Duration: 14 days (max 30 days)
  ðŸ“ˆ Adaptive approach based on historical performance
```

For more example outputs, see the [`examples/`](examples/) directory.

---

## Key Concepts

### Agent Flow - Detailed

**Initialize Path** (New project with historical data):
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Load Context â”‚  Check if project exists in DB
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Analyze Filesâ”‚  Detect file type: historical/experiments/enrichment
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Router    â”‚  LLM decides: "initialize" (new project + historical data)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Discovery  â”‚  Intelligent discovery: LLM inference + web search + user prompts
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Data Collect â”‚  Merge files â†’ historical_data, get detailed analysis
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Insight   â”‚  Generate strategy (insights, audience, creative, platform)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  + execution timeline (7-30 days, adaptive)
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Campaign     â”‚  Generate TikTok + Meta configs with targeting & creative
â”‚Setup        â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Save State  â”‚  Persist to Supabase (projects table)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â†“
      END
```

**Reflect Path** (Existing project with experiment results):
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Load Context â”‚  Restore project state from DB
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Analyze Filesâ”‚  Detect file type: experiment_results
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Router    â”‚  LLM decides: "reflect" (experiment results uploaded)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Reflection  â”‚  Analyze performance vs thresholds
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  Identify top performers & underperformers
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Adjustment  â”‚  Generate optimization patch
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  Create new config (v1, v2, v3, ...)
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Save State  â”‚  Persist updated state + new config
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â†“
      END
```

### State Persistence

All agent state is saved to Supabase after each session:
- Historical data and market benchmarks
- Strategy and experiment plans
- Campaign configurations (all versions)
- Patch history and performance metrics
- Decision reasoning and insights

### LLM Router Logic

The router examines:
1. Project exists? (project_loaded)
2. File types? (historical/experiments/enrichment)
3. Current phase? (initialized/strategy_built/awaiting_results/optimizing)

Decision outcomes:
- **initialize**: No project â†’ full setup flow
- **reflect**: Experiment results â†’ performance analysis + patches
- **enrich**: Additional data â†’ update strategy
- **continue**: Resume incomplete flow

## Development

### Adding New Nodes

1. Implement node function in `src/agent/nodes.py`
2. Add node to graph in `src/agent/graph.py`
3. Update router logic if needed in `src/agent/router.py`

### Testing

```bash
# Run with sample data
python cli.py run --project-id test-001
> Files: examples/sample_historical.csv
```

### Debugging

Check `react_cycles` table for complete execution trace:
```sql
SELECT * FROM react_cycles
WHERE project_id = 'your-project-id'
ORDER BY timestamp DESC;
```

## Roadmap

### Phase 2 (Future)
- Real-time performance monitoring
- API integration with TikTok/Meta Ads Manager
- Auto-execution of campaign updates
- Human-in-the-loop approval workflows

### Phase 3 (Advanced)
- Predictive performance modeling
- Cross-campaign pattern detection
- Portfolio-level optimization
- Advanced semantic memory with embeddings

## Troubleshooting

**Import errors**: Ensure virtual environment is activated and dependencies installed

**Database connection errors**: Verify Supabase credentials in `.env`

**LLM errors**: Check Gemini API key and quota

**File parsing errors**: Ensure CSV/JSON files have required columns (see File Formats section)

---

## For Collaborators

### Documentation

- **[ARCHITECTURE.md](ARCHITECTURE.md)** - Comprehensive technical documentation covering:
  - System design principles
  - LangGraph agent architecture (detailed node descriptions)
  - State management patterns
  - Router decision logic
  - Module breakdown (insight.py, campaign.py, reflection.py, etc.)
  - LLM integration guidelines
  - Database schema
  - Development patterns & best practices

- **[CONTRIBUTING.md](CONTRIBUTING.md)** - Collaboration guidelines:
  - Development setup
  - Code organization
  - Adding new nodes
  - Testing guidelines
  - Pull request process

- **[examples/](examples/)** - Sample data and outputs:
  - Sample CSV files (historical data, experiment results)
  - Sample JSON outputs (strategy, timeline, configs)
  - Annotated examples with explanations

### Quick Start for Developers

1. **Clone and setup**:
```bash
git clone <repo-url>
cd adronaut-agent
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
cp .env.example .env
# Edit .env with your API keys
```

2. **Read the docs**:
- Start with this README for overview
- Read [ARCHITECTURE.md](ARCHITECTURE.md) for technical deep dive
- Check [examples/](examples/) for sample data

3. **Run with sample data**:
```bash
python cli.py run --project-id test-campaign
# Upload: examples/sample_historical_data.csv
```

4. **Explore the codebase**:
- `src/agent/graph.py` - Workflow definition
- `src/agent/nodes.py` - Node implementations
- `src/modules/` - Business logic modules
- `cli.py` - Entry point and output formatting

### Key Features for Contributors

- **Flexible execution timelines**: See `src/modules/execution_planner.py` - LLM-powered adaptive 7-30 day test plans
- **Intelligent discovery**: See `src/agent/nodes.py:discovery_node` - Parallel strategies (LLM + web search + user prompts)
- **Knowledge graph**: See `src/agent/state.py:knowledge_facts` - Confidence-scored facts with provenance
- **Multi-session state**: See `src/database/persistence.py` - Full state restoration across sessions
- **Progress tracking**: See `src/utils/progress.py` - Real-time node execution logging

### Architecture Highlights

```
Router-Based Workflow:
- LLM examines project state â†’ decides next action
- Supports: initialize | reflect | enrich | continue

State-First Design:
- Single AgentState dict flows through graph
- Full visibility, resumability, debugging

Separation of Concerns:
- Nodes: Orchestration & state updates
- Modules: Pure business logic functions
- LLM: Centralized reasoning engine
```

## License

MIT

## Contributing

We welcome contributions! To get started:

1. Read [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines
2. Check [GitHub Issues](https://github.com/your-repo/issues) for open tasks
3. Fork the repo and create a feature branch
4. Make your changes and add tests
5. Submit a pull request with clear description

For questions or discussions, open an issue or reach out to the maintainers.
